# NMT by Jointly Learning to Algin and Translate (Using Seq2seq + Attention)

Sequence to sequence networks are powerful methods to use two recurrent neural networks to turn one sequence into another; in this case,
a French-English translation.

This code is an implementation &  modification of a tutorial given by PyTorch tutorials in "Translation with a Sequence to Sequence Network and Attention" in the following link:

	https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html


## References

For further studies about word embeddings, read the papers below:

1. NMT by Jointly Learning to Algin and Translate (Bahdanau et al.)
2. Sequence to Sequence Learning with Neural Networks (Sutskever et al.)



 
